#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ç–µ—Å—Ç–µ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤ AI –º–µ–¥–∏–∞—Ç–æ—Ä–∞.
–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –±–µ–∑ –ø–æ–¥–Ω—è—Ç–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤.

–£—Å—Ç–∞–Ω–æ–≤–∫–∞: uv sync --extra tools
–ó–∞–ø—É—Å–∫: python tools/prompt_tester.py --all
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

import click
import yaml
from openai import OpenAI
from pydantic import BaseModel
from rich.console import Console
from rich.table import Table
from rich.progress import track

# –î–æ–±–∞–≤–ª—è–µ–º shared –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent / "shared"))

console = Console()


class TestResult(BaseModel):
    test_name: str
    status: str  # "passed", "failed", "error"
    score: float  # 0.0 - 1.0
    details: Dict[str, Any]
    execution_time: float


class PromptTester:
    def __init__(self, openai_api_key: str):
        self.client = OpenAI(api_key=openai_api_key)
        self.prompts_dir = Path(__file__).parent.parent / "shared" / "prompts"
        self.tests_dir = self.prompts_dir / "tests" / "cases"
        self.results_dir = self.prompts_dir / "tests" / "results"
        self.results_dir.mkdir(exist_ok=True)

    def load_current_prompt(self) -> str:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç –º–µ–¥–∏–∞—Ç–æ—Ä–∞"""
        mediator_prompt = self.prompts_dir / "current" / "mediator_v1.md"
        system_rules = self.prompts_dir / "current" / "system_rules.md"

        prompt_parts = []

        if mediator_prompt.exists():
            prompt_parts.append(mediator_prompt.read_text(encoding="utf-8"))

        if system_rules.exists():
            prompt_parts.append(system_rules.read_text(encoding="utf-8"))

        return "\n\n".join(prompt_parts)

    def load_test_case(self, test_file: Path) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –∫–µ–π—Å"""
        with open(test_file, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    def format_messages_for_prompt(self, messages: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        formatted = []
        for msg in messages:
            timestamp = msg["timestamp"]
            user_id = msg["user_id"]
            content = msg["content"]
            formatted.append(f"[{timestamp}] {user_id}: {content}")

        return "\n".join(formatted)

    def run_test_case(self, test_case: Dict[str, Any]) -> TestResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –∫–µ–π—Å"""
        start_time = datetime.now()

        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç
            system_prompt = self.load_current_prompt()
            messages_text = self.format_messages_for_prompt(test_case["messages"])

            user_prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–∞—Ä—ã (pair_id: {test_case["pair_id"]}):

            {messages_text}

            –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.
            """

            # –í—ã–∑—ã–≤–∞–µ–º OpenAI API
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.7,
                max_tokens=2048,
            )

            ai_response = response.choices[0].message.content

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
            score = self.evaluate_response(ai_response, test_case)

            execution_time = (datetime.now() - start_time).total_seconds()

            return TestResult(
                test_name=test_case["name"],
                status="passed" if score >= 0.7 else "failed",
                score=score,
                details={
                    "ai_response": ai_response,
                    "expected": test_case.get("expected_output", {}),
                    "evaluation_notes": self.get_evaluation_notes(
                        ai_response, test_case
                    ),
                },
                execution_time=execution_time,
            )

        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            return TestResult(
                test_name=test_case.get("name", "Unknown"),
                status="error",
                score=0.0,
                details={"error": str(e)},
                execution_time=execution_time,
            )

    def evaluate_response(self, response: str, test_case: Dict) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)"""
        score = 0.0
        expected = test_case.get("expected_output", {})

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è –æ–±–æ–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        participants = test_case.get("messages", [])
        unique_users = set(msg["user_id"] for msg in participants)

        users_mentioned = 0
        for user_id in unique_users:
            if user_id.lower() in response.lower():
                users_mentioned += 1

        if users_mentioned == len(unique_users):
            score += 0.3

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ expected
        if "participants" in expected:
            for user_data in expected["participants"].values():
                should_include = user_data.get("should_include", [])
                for keyword in should_include:
                    if keyword.lower() in response.lower():
                        score += 0.1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        if "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏" in response.lower() or "—Å–æ–≤–µ—Ç—ã" in response.lower():
            score += 0.2

        if "–ø–æ–Ω–∏–º–∞—é" in response.lower() or "—á—É–≤—Å—Ç–≤—É–µ—à—å" in response.lower():
            score += 0.2

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if "participants" in expected:
            for user_data in expected["participants"].values():
                should_avoid = user_data.get("should_avoid", [])
                for keyword in should_avoid:
                    if keyword.lower() in response.lower():
                        score -= 0.1

        return min(1.0, max(0.0, score))

    def get_evaluation_notes(self, response: str, test_case: Dict) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–∞–º–µ—Ç–∫–∏ –ø–æ –æ—Ü–µ–Ω–∫–µ"""
        notes = []

        if len(response) < 200:
            notes.append("–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")

        if "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏" not in response.lower():
            notes.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —è–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

        if response.count("user_") != 2:
            notes.append("–ù–µ —É–ø–æ–º—è–Ω—É—Ç—ã –æ–±–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")

        return notes

    def save_results(self, results: List[TestResult]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"test_results_{timestamp}.json"

        results_data = {
            "timestamp": timestamp,
            "total_tests": len(results),
            "passed": len([r for r in results if r.status == "passed"]),
            "failed": len([r for r in results if r.status == "failed"]),
            "errors": len([r for r in results if r.status == "error"]),
            "average_score": sum(r.score for r in results) / len(results)
            if results
            else 0,
            "results": [r.dict() for r in results],
        }

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)

        console.print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
        return results_file


@click.command()
@click.option("--test-case", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–µ—Å—Ç")
@click.option("--all", "run_all", is_flag=True, help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ç–µ—Å—Ç—ã")
@click.option("--api-key", envvar="OPENAI_API_KEY", help="OpenAI API –∫–ª—é—á")
def main(test_case: str, run_all: bool, api_key: str):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ AI –º–µ–¥–∏–∞—Ç–æ—Ä–∞"""

    if not api_key:
        console.print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è OPENAI_API_KEY", style="red")
        sys.exit(1)

    tester = PromptTester(api_key)

    # –ù–∞—Ö–æ–¥–∏–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    test_files = []
    if test_case:
        test_file = tester.tests_dir / f"{test_case}.yaml"
        if test_file.exists():
            test_files = [test_file]
        else:
            console.print(f"‚ùå –¢–µ—Å—Ç {test_case} –Ω–µ –Ω–∞–π–¥–µ–Ω", style="red")
            sys.exit(1)
    elif run_all:
        test_files = list(tester.tests_dir.glob("*.yaml"))
    else:
        console.print("–£–∫–∞–∂–∏—Ç–µ --test-case –∏–ª–∏ --all")
        sys.exit(1)

    if not test_files:
        console.print("‚ùå –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã", style="red")
        sys.exit(1)

    console.print(f"üöÄ –ó–∞–ø—É—Å–∫ {len(test_files)} —Ç–µ—Å—Ç–æ–≤...")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    results = []
    for test_file in track(test_files, description="–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤..."):
        test_case_data = tester.load_test_case(test_file)
        result = tester.run_test_case(test_case_data)
        results.append(result)

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    table = Table(title="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤")
    table.add_column("–¢–µ—Å—Ç")
    table.add_column("–°—Ç–∞—Ç—É—Å")
    table.add_column("–û—Ü–µ–Ω–∫–∞")
    table.add_column("–í—Ä–µ–º—è")

    for result in results:
        status_color = (
            "green"
            if result.status == "passed"
            else "red"
            if result.status == "failed"
            else "yellow"
        )
        table.add_row(
            result.test_name,
            f"[{status_color}]{result.status}[/{status_color}]",
            f"{result.score:.2f}",
            f"{result.execution_time:.1f}s",
        )

    console.print(table)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    tester.save_results(results)

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    passed = len([r for r in results if r.status == "passed"])
    total = len(results)
    avg_score = sum(r.score for r in results) / total if results else 0

    console.print(
        f"\nüìä –ò—Ç–æ–≥–∏: {passed}/{total} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ, —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}"
    )


if __name__ == "__main__":
    main()
